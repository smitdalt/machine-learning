{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "341cc156b678b0918a022729669f6afa0305e468"
   },
   "source": [
    "### Supervised Learning: Linear Regression - Feature and Model Selection\n",
    "\n",
    "Jay Urbain, PhD\n",
    "\n",
    "We will be working with the`Bike Sharing Demand` dataset from a prior Kaggle competition:  \n",
    "https://www.kaggle.com/c/bike-sharing-demand \n",
    "\n",
    "**Assignment objective:** perform: \n",
    "- data analysis and visualization      \n",
    "- preprocessing and feature engineering \n",
    "- fit a uni-variate linear regression model   \n",
    "- fit a mutli-variate linear regression model   \n",
    "- evauate the importances of each feature  \n",
    "- identify a subset of features to maximize model performance .  \n",
    "\n",
    "**Dataset Description:**\n",
    "Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.\n",
    "\n",
    "The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.\n",
    "\n",
    "**Kaggle Objective:** \n",
    "You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\n",
    "\n",
    "**Data Fields:**  \n",
    "**datetime** - hourly date + timestamp    \n",
    "**season** - 1: spring, 2: summer, 3: fall, 4: winter    \n",
    "**holiday** - whether the day is (1) or is not (0) considered a holiday   \n",
    "**workingday** - whether the day is neither a weekend nor holiday   \n",
    "**weather** -    \n",
    "&nbsp;&nbsp;1: Clear, Few clouds, Partly cloudy, Partly cloudy    \n",
    "&nbsp;&nbsp;2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist    \n",
    "&nbsp;&nbsp;3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds      \n",
    "&nbsp;&nbsp;4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "**temp** - temperature in Celsius   \n",
    "**atemp** - \"feels like\" temperature in Celsius   \n",
    "**humidity** - relative humidity   \n",
    "**windspeed** - wind speed   \n",
    "**casual** - number of non-registered user rentals initiated   \n",
    "**registered** - number of registered user rentals initiated   \n",
    "**count** - number of total rentals  (casual + registered) .  \n",
    "\n",
    "**References**:   \n",
    "- Fanaee-T, Hadi, and Gama, Joao, Event labeling combining ensemble detectors and background knowledge, Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg.\n",
    "- https://www.kaggle.com/c/bike-sharing-demand (Kaggle site) \n",
    "- James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning, with applications in R, www.StatLearning.com, Springer-Verlag, New York. Chapter 4\n",
    "- [scikit-learn](http://scikit-learn.org/stable/)  \n",
    "- https://medium.com/@viveksrinivasan/how-to-finish-top-10-percentile-in-bike-sharing-demand-competition-in-kaggle-part-1-c816ea9c51e1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c2704feebb8fcb6a8693a8f5c300e0d5c9f864a"
   },
   "source": [
    "Carefully execute each cell in the notebook. Perform tasks or answer questions in the cell(s) below each bolded **TODO** in the in the notebook. Its important to carefully read each cell before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries\n",
    "\n",
    "*Note: If you haven't done so, make sure you create a conda environment (see week 1 notebook) and install your libraries after selecting that environment.*   \n",
    "\n",
    "For this assignment you will need the following python libraries:  \n",
    "- numpy  \n",
    "- matplotlib   \n",
    "- pandas   \n",
    "- seaborn  \n",
    "- sklearn   \n",
    "\n",
    "It is customary to import your libraries towards the top of your notebook. If you are missing any of the libraries you can install them using:  \n",
    "> conda install `library_name`  \n",
    "or install from a specific collection:  \n",
    "conda install -c anaconda `library_name`  \n",
    "or using pip:  \n",
    "pip install `library_name` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3714683c93de6fe9db52ff4973510692a1d10b33"
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting package\n",
    "import seaborn as sns           # more specialized plotting package built on matplotlib\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "# generate plots within notebook rather than external window\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd # data frame, dataprocessing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV # regression models\n",
    "\n",
    "#evaluation metrics\n",
    "from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"data/bike-sharing-dataset\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mac or linux\n",
    "!ls data/bike-sharing-dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the train.csv and test.csv files generated for the Kaggle competition.\n",
    "\n",
    "Create Pandas dataframes for the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29ee14f4c117e6ac694e37305a46a8d56a118822",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(r'data/bike-sharing-dataset/train.csv')\n",
    "test=pd.read_csv(r'data/bike-sharing-dataset/test.csv')\n",
    "df_train=train.copy()\n",
    "df_test=test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape attribute of the data provides a tuple of the dataframes dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e374b33b0947c5e71b524344ca98972028b4e80"
   },
   "source": [
    "Why does the test set have 3 fewer columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3209eb63e926c5a9a172ac856e8cbe234be5bc08"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Since this is a Kaggle competition, the test set provided does not include target variables for prediction. Note how `casual` + `registered` = `count`. We will need to take care and not include them as a feature in our model.\n",
    "\n",
    "For this exercise, we will need to construct our own test data from the training data supplied. If you would like to sumbit your results to Kaggle, you would use the supplied test data and evaluate your model on *root mean squared log error.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22a691d4be92d23a7830a98adb9bae2de71e6f78"
   },
   "source": [
    "The info() method provides counts and data types for each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aae2ea236fd1d3a70f4444d507eed5026d1841e3"
   },
   "outputs": [],
   "source": [
    "df = df_train\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get to the type information using the 'dtypes' field which accesses the underlying numpy type for each column (Pandas Series) in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important preprocessing step is to determine if we have any missing or null values. If we do, we need to either remove them or impute some missing value.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/impute.html  \n",
    "\n",
    "If the sum is zero, we have not problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ede9438401bf029bfddbddee29116b0c399b6c3e"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory feature analysis\n",
    "\n",
    "We should evaluate each predictor (feature) indvidually and with respect to the target variable `count`. We are interested in the distribution of the data and whether there is any correlation between features and the target variable.\n",
    "\n",
    "For categorical data we can use value counts per category for a quantitative estimate. And either a barplot with value counts or a factorplot which counts values automatically to gain a better understanding of the distribution of each category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of categorical variables**\n",
    "\n",
    "Lets start with `season`\n",
    "\n",
    "season - 1: spring, 2: summer, 3: fall, 4: winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80c0b0eeb79267103d7e0f47e11cd0dd787a8393"
   },
   "outputs": [],
   "source": [
    "df.season.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Seaborn categorical plot (formerly called a factor plot) to show this distribution graphically.\n",
    "\n",
    "https://seaborn.pydata.org/tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "07a59826be47b0c10e8aa41955907262d0e0178c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = sns.catplot(x='season', data=df, kind='count', height=3, aspect=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:* The samples are evenly distributed across seasons. This is good. We will not have to take special steps to deal with an unbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Holiday`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a84974a01956d6e8ae5492f88d763211e1b4aaee"
   },
   "outputs": [],
   "source": [
    "#holiday\n",
    "print( df_train.holiday.value_counts() )\n",
    "sns.catplot(x='holiday',data=df,kind='count',height=3,aspect=1.5) # majority of data is for non holiday days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:* The vast majority of samples are non-holiday work days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Weather`\n",
    "\n",
    "**weather** -    \n",
    "&nbsp;&nbsp;1: Clear, Few clouds, Partly cloudy, Partly cloudy    \n",
    "&nbsp;&nbsp;2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist    \n",
    "&nbsp;&nbsp;3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds      \n",
    "&nbsp;&nbsp;4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "\n",
    "**TODO** Generate counts, a plot, and record your observations for weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4469180f5a72e11927ca8969c83672259bf824fc"
   },
   "outputs": [],
   "source": [
    "# weather counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical weather plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*   \n",
    "\n",
    "Highest demand in nice weather. This feature (weather) is likely to be predictive of the target variable `count`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`temp` - temperature in Celsius   \n",
    "\n",
    "**TODO** Generate counts, a plot, and record your observations for temp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp counts    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "693e609795fbec7681820985bccaec9fb629fecf"
   },
   "outputs": [],
   "source": [
    "# Categorical plot \n",
    "# 1-> spring\n",
    "# 2-> summer\n",
    "# 3-> fall\n",
    "# 4-> winter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the relationship of categorical attributes with resect to numeric (continuous) variables such as our target `count` with a categorical box plot.\n",
    "\n",
    "https://seaborn.pydata.org/tutorial/categorical.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"season\", y=\"count\", kind=\"box\", data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: What can you observe about the distribution of `count` (number of riders) for each season? Can you come up with a hypothesis to explain the outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of continuous variables.**\n",
    "\n",
    "We can use the dataframe to `describe()` method to quantify the distribution of continuous variables. *Note: these measures do not make a lot of sense for categorical variables.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show the continuous variable distributions with a single box plot.\n",
    "\n",
    "The box plot shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the inter-quartile range.\n",
    "\n",
    "https://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
    "\n",
    "Lets generate a boxplot for our target variable `count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"count\", data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** From the boxplot for `count` above, identify the approximate median value.\n",
    "Also identify if the data is normally distributed and if there are any significant outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate boxplots for each of the following numerical attributes. \n",
    "\n",
    "`temp` - temperature in Celsius   \n",
    "`atemp` - \"feels like\" temperature in Celsius   \n",
    "`humidity` - relative humidity   \n",
    "`windspeed` - wind speed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to visualize.\n",
    "sns.boxplot(data=df[['temp', 'atemp', 'humidity', 'windspeed']])\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "Record your observations for each continuous variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view distributions of continuous variables with histograms.\n",
    "\n",
    "https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6be3e1df01d7fe136efc717480e08f2d05c4e3dd"
   },
   "outputs": [],
   "source": [
    "# can also be visulaized using histograms for all the continuous variables.\n",
    "df.temp.unique()\n",
    "fig,axes=plt.subplots(2,2)\n",
    "axes[0,0].hist(x=\"temp\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\n",
    "axes[0,0].set_title(\"Variation of temp\")\n",
    "axes[0,1].hist(x=\"atemp\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\n",
    "axes[0,1].set_title(\"Variation of atemp\")\n",
    "axes[1,0].hist(x=\"windspeed\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\n",
    "axes[1,0].set_title(\"Variation of windspeed\")\n",
    "axes[1,1].hist(x=\"humidity\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\n",
    "axes[1,1].set_title(\"Variation of humidity\")\n",
    "fig.set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing statistical relations**\n",
    "\n",
    "After reviewing the distributions for categorical and continuous variables. We would like to visualize relations between variables. Especially between each predictor and the target.\n",
    "\n",
    "https://seaborn.pydata.org/generated/seaborn.heatmap.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop casual and registred since count + registred = count\n",
    "df.drop(['casual','registered'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52d92f76809b32f84e936a57ba582c9188aba311"
   },
   "outputs": [],
   "source": [
    "#correlation matrix.\n",
    "cor_mat= df[:].corr()\n",
    "mask = np.array(cor_mat)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(30,12)\n",
    "x = sns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**  \n",
    "1) Identify ~3 pairs of variables with the highest correlation.    \n",
    "2) Identify `3 predictor variables that have the highest correlation with the target variable `count`.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d55facfabff7b9b685dca09bac2b6607df12d8e"
   },
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "First, we have to replace categorical values with either the category type or one-hot (dummie encoding).\n",
    "\n",
    "We can use the Pandas `get_dummies()` method:   \n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html\n",
    "\n",
    "Alternatively, there is the scikit-learn `OneHotEncoder`.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d5d88ba071ae70848b9d5649ff1b29d5afaffc5"
   },
   "outputs": [],
   "source": [
    "# one-hot encoding for season\n",
    "\n",
    "season=pd.get_dummies(df['season'],prefix='season')\n",
    "df=pd.concat([df,season],axis=1)\n",
    "df.head()\n",
    "season=pd.get_dummies(df_test['season'],prefix='season')\n",
    "df_test=pd.concat([df_test,season],axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Perform one-hot encoding for weather on df and df_test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41cd4105b035ed61f789686bc1bba906f092496b"
   },
   "outputs": [],
   "source": [
    "# one-hot encode weather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b98082ac0dd6d032e8b0094b024ceb2d00097b8a"
   },
   "outputs": [],
   "source": [
    "# we can now can drop weather and season.\n",
    "df.drop(['season','weather'],inplace=True,axis=1)\n",
    "df.head()\n",
    "df_test.drop(['season','weather'],inplace=True,axis=1)\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7bfc03e0568857a40a2561b0566913103c0752c4"
   },
   "source": [
    "Split the date and time, as the time of day is expected to effect the number of bikes. For example, rush hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e997d136e4b03fc9dd5eee8667da39ac6cfd9e6"
   },
   "outputs": [],
   "source": [
    "df[\"hour\"] = [t.hour for t in pd.DatetimeIndex(df.datetime)]\n",
    "df[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(df.datetime)]\n",
    "df[\"month\"] = [t.month for t in pd.DatetimeIndex(df.datetime)]\n",
    "df['year'] = [t.year for t in pd.DatetimeIndex(df.datetime)]\n",
    "df['year'] = df['year'].map({2011:0, 2012:1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "950fcc8ea72778c7f1963ee6106f4a68649974d4"
   },
   "outputs": [],
   "source": [
    "df_test[\"hour\"] = [t.hour for t in pd.DatetimeIndex(df_test.datetime)]\n",
    "df_test[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(df_test.datetime)]\n",
    "df_test[\"month\"] = [t.month for t in pd.DatetimeIndex(df_test.datetime)]\n",
    "df_test['year'] = [t.year for t in pd.DatetimeIndex(df_test.datetime)]\n",
    "df_test['year'] = df_test['year'].map({2011:0, 2012:1})\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d04fa7dc6c3d03f70ba6b53abcb74b492f9c0739"
   },
   "outputs": [],
   "source": [
    "# drop datetime column.\n",
    "df.drop('datetime',axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33a931615968a6aee0fdf1381d968de1d9844720"
   },
   "source": [
    "#### Review our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cfaec3adda96ef23da53953b518d6af02d62865f"
   },
   "outputs": [],
   "source": [
    "cor_mat= df[:].corr()\n",
    "mask = np.array(cor_mat)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(30,12)\n",
    "sns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bbd11afa67d731e3749229861f79f0de5ea7729e"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify we haven't inadvertantly added any null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-learn for multivariate regression\n",
    "\n",
    "*Scikit-learn* can be used for univariate or multivariate regression.\n",
    "\n",
    "The [sklearn.linear_model.LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class is called an estimator. \n",
    "\n",
    "Estimators predict a value based on the observed data. In scikit-learn, all estimators implement the *fit()* and *predict()* methods. The *fit()* method is used to learn the parameters of a model, and the *predict()* method is used to predict the value of a response variable for a given predictor variable using the learned coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Linear Regression\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Scikit-learn Linear Model Documentation](http://scikit-learn.org/stable/modules/linear_model.html).\n",
    "\n",
    "$ y(\\beta,x) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p $\n",
    "\n",
    "Where $\\beta = (\\beta_1, ...\\beta_p)$ are the coefficients and $ \\beta_0 $ as the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** .  \n",
    "Create a LinearRegression object.\n",
    "\n",
    "*Reminder: After typing in an object, you can press tab to see a list of methods*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LinearRegression Object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression functions:\n",
    "\n",
    "- lreg.fit(): fits a linear model\n",
    "\n",
    "- lreg.predict(): predict Y from X using the linear regression model coefficients\n",
    "\n",
    "- lreg.score(): returns $R^2$, the [coefficient of determination](http://en.wikipedia.org/wiki/Coefficient_of_determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the bike sharing dataframe into feature (predictor) columns and the target column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Columns\n",
    "X_multi = df.drop('count',1)\n",
    "\n",
    "# Targets\n",
    "Y_target = df['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Fit the multi-variate linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the intercept and the number of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' The estimated intercept coefficient is %.2f ' %lreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' The number of coefficients used was %d ' % len(lreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame to examine the model and the estimated coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the Features\n",
    "coeff_df = pd.DataFrame(df.columns)\n",
    "coeff_df.columns = ['Features']\n",
    "\n",
    "# Add a new column with the coefficients from the linear regression\n",
    "coeff_df[\"Coefficient Estimate\"] = pd.Series(lreg.coef_)\n",
    "\n",
    "# Show\n",
    "print ('Table 1. Multivariate coefficients')\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Which coefficients, have the strongest correlation with respect to the target variable. Use the cell below. *Note: consider one-hot encoded values as one variable, i.e., season and weather*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Validation Sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate training and test sets should be used to train and validate the performance of the model.\n",
    "\n",
    "Samples for each set should be randomly selected.\n",
    "\n",
    "Fortunately, scikit-learn has a built in function specifically for this called `train_test_split()`.\n",
    "\n",
    "Below, we are creating separate training and tests sets, holding out 20% of the data for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X_multi, Y_target, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results of the data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes of the training and testing data sets\n",
    "# print (X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "# #numpy expects matrix with dimension column\n",
    "# X_train = X_train.values.reshape(X_train.shape[0],1)\n",
    "# Y_train = Y_train.values.reshape(Y_train.shape[0],1)\n",
    "# X_test = X_test.values.reshape(X_test.shape[0],1)\n",
    "# Y_test = Y_test.values.reshape(Y_test.shape[0],1)\n",
    "\n",
    "print (X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our training set to build the model, and the test set to evaluate the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression object\n",
    "lreg = LinearRegression(normalize=True)\n",
    "\n",
    "# Build a linear regression model on the training data only\n",
    "lreg.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform prediction on both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on training and testing sets\n",
    "pred_train = lreg.predict(X_train)\n",
    "pred_test = lreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** \n",
    "\n",
    "Calculate the root mean square error on your training set Y_train and your test set Y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE \n",
    "\n",
    "print (\"Fit a model X_train, and calculate RMSE with Y_train: %.2f\"  % np.sqrt(np.mean((Y_train - pred_train) ** 2)) )\n",
    "    \n",
    "print (\"Fit a model X_train, and calculate RMSE with X_test and Y_test: %.2f\"  % np.sqrt(np.mean((Y_test - pred_test) ** 2)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Build a univariate model with a feature that you believe is highly correlated with `count'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate model in the cells below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** \n",
    "\n",
    "Calculate the root mean square error on your training set Y_train and your test set Y_test for your univariate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Does our multivariate linear regression model reduce the RMSE with respect to the univariate model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Does our multivariate linear regression model explain more the variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Plots\n",
    "\n",
    "A residual plot is a graph that shows the residuals on the vertical axis and the independent variable (x) on the horizontal axis. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a non-linear model is more appropriate.\n",
    "\n",
    "Residual plots are a good way to visualize the errors in your data. A good model fit will show data points randomly and evenly scattered around line zero. If there is some strucutre or pattern, that means your model is not capturing some aspect of the data. There could be an interaction between predictor variables that we are not considering, or the data may be inherently non-linear.\n",
    "\n",
    "[Residual plots](http://blog.minitab.com/blog/adventures-in-statistics/why-you-need-to-check-your-residual-plots-for-regression-analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot the training data\n",
    "train = plt.scatter(pred_train,(pred_train-Y_train),c='b',alpha=0.5)\n",
    "\n",
    "# Scatter plot the testing data\n",
    "test = plt.scatter(pred_test,(pred_test-Y_test),c='r',alpha=0.5)\n",
    "\n",
    "# Plot a horizontal axis line at 0\n",
    "plt.hlines(y=0,xmin=-10,xmax=50)\n",
    "\n",
    "#Labels\n",
    "plt.legend((train,test),('Training','Test'),loc='lower left')\n",
    "plt.title('Residual Plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a broad topic with many applications. More information can be found in the scikit-lear documentation:  http://scikit-learn.org/stable/modules/linear_model.html#linear-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Review Table 1. Multivariate coefficients. Think about the meaning of a linear regression model, i.e., the \n",
    "coefficient reflects the change in the target variable for a one unit change in an input variable, with all other \n",
    "variables held constant. Identify a subset of features and build a model with these features. See if you can reduce \n",
    "RMSE and increase $R^2$.* \n",
    "\n",
    "Document your results in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination\n",
    "\n",
    "Feature ranking with recursive feature elimination.\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html\n",
    "\n",
    "http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Perform RFE or RFECV using the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.svm import SVR\n",
    "# X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "lr = LinearRegression()\n",
    "selector = RFE(lr, n_features_to_select=5)\n",
    "selector = selector.fit(X_train,Y_train)\n",
    "print( selector.support_ )\n",
    "print( selector.ranking_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_multi.columns[selector.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_multi[ X_multi.columns[selector.ranking_==1]][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_multi_imp = X_multi[ X_multi.columns[selector.ranking_==1]]\n",
    "X_multi_imp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imp, X_test_imp, Y_train_imp, Y_test_imp = sklearn.model_selection.train_test_split(X_multi_imp, Y_target, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression object\n",
    "lreg = LinearRegression()\n",
    "\n",
    "# Build a linear regression model on the training data only\n",
    "lreg.fit(X_train_imp,Y_train_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on training and testing sets\n",
    "pred_train = lreg.predict(X_train_imp)\n",
    "pred_test = lreg.predict(X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Fit a model X_train, and calculate RMSE with Y_train: %.2f\"  % np.sqrt(np.mean((Y_train_imp - pred_train) ** 2)) )\n",
    "    \n",
    "print (\"Fit a model X_train, and calculate RMSE with X_test and Y_test: %.2f\"  % np.sqrt(np.mean((Y_test_imp - pred_test) ** 2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(Y_test_imp, pred_test) \n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2 = 1 - \\frac{RSS}{TSS}$ \n",
    "\n",
    "$Adjusted R^2 = 1 - \\frac{RSS/(n-d-1)}{TSS/(n-d)}$ \n",
    "\n",
    "Scikit-learn provides an $R^2$ measurement, but not Adjusted $R^2$. You can calculate  Adjusted $R^2$ from $R^2@ as follows:\n",
    "\n",
    "$Adjusted R^2 = 1 - (1-R^2)*\\frac{(n-1)}{(n-d-1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_train_imp.shape[0]\n",
    "p = X_train_imp.shape[1]\n",
    "adj_r2 = 1-(1-R2)*(n-1)/(n-p-1)\n",
    "adj_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Using RFE, identify the optimal number of features based on Adjusted $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work in the cell(s) below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Using RFE, identify the optimal number of features using `LinearRegression()` based on Adjusted $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work in the cell(s) below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Using the features identified using RFE for `LinearRegression()` and the same *train-test-spit* of data, compare the performance of `LinearRegression()`, `Ridge()`,  `Lasso()`, and `RandoizedLasso()` based on Adjusted $R^2$.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.linear_model.RandomizedLasso.html\n",
    "\n",
    "*Note:* you will need to tune your value of $\\alpha$ for ridge and lasso regreesion. I would recommend trying ~3 values for $\\alpha$. The default is $1$.\n",
    "\n",
    "There is a very nice tutorial on tuning hyperparameters for regression here:    \n",
    "https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n",
    "\n",
    "Scikit-learn has a `GridSearch` function with built in cross-validation. You can use this to evaluate multiple paramters automatically. This is not required.   \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "If you're interested you can also evaluate more advanced regressors: RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor. This is not required. These regressors will be discuissed a little later in the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work in the cell(s) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
